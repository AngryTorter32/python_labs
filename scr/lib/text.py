def normalize(text, casefold, yo2e):
    if casefold != False:
        text = text.casefold()
    if yo2e != False:
        text = text.replace('—ë', '–µ')
        text = text.replace('–Å', '–ï')
    text = text.replace('\n', ' ')
    text = text.replace('\t', ' ')
    text = text.replace('\r', ' ')
    text = text.replace('\b', ' ')
    spis = text.split()
    text = ''
    for i in range(len(spis)):
        text += spis[i]
        text += ' '
    text = text.strip()
    return text
def tokenize(text):
    raz = ['!', ',', 'üòÄ', '‚Äî', ':', ';', '?', '/', '&', '*', '#', '$', '%']
    for i in range(len(text)):
        if text[i] in raz:
            text = text.replace(text[i], ' ')
    spis = text.split()
    return spis
def count_freq(text):
    sl = set(text)
    sl = list(sl)
    kol = []
    for i in range(len(sl)):
        a = str(sl[i]) + ':' + str(text.count(sl[i]))
        kol.append(a)
    return kol
print('normalize:')
print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", True, True), normalize('—ë–∂–∏–∫, –Å–ª–∫–∞', True, True), sep = '\n')
print(normalize("Hello\r\nWorld", True, True), normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", True, True), sep='\n')
print('', 'tokenize:', sep = '\n')
print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"), tokenize("hello,world!!!"), sep = '\n')
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"), tokenize("2025 –≥–æ–¥"), sep = '\n')
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
print(' ', 'count_freq + top_n:', sep = '\n')
print(count_freq(["a","b","a","c","b","a"]))
